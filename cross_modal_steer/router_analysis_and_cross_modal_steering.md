# Router Implementation Analysis and Cross-Modal Steering Analysis

## 1. Router Implementation Comparison

### Current Implementation (SteerMoE Aligner)
```python
class SteerMoEAligner(nn.Module):
    def __init__(self, feature_dim, num_experts):
        self.steering_vectors = nn.Parameter(torch.randn(num_experts, feature_dim))
        self.router = nn.Linear(feature_dim, num_experts)
    
    def forward(self, h_audio):
        gating_logits = self.router(h_audio)  # (batch, seq_len, num_experts)
        gating_scores = F.softmax(gating_logits, dim=-1)
        delta_h = torch.einsum('bte,ef->btf', gating_scores, self.steering_vectors)
        return h_audio + delta_h
```

**Pros:**
- Simple and efficient
- Low computational overhead
- Easy to train and debug
- Works well for post-encoder alignment

**Cons:**
- Single routing decision per token
- No hierarchical structure
- Limited expressiveness for complex alignment patterns
- No temporal or spatial awareness

### Hierarchical Router Design
```python
class HierarchicalRouter(nn.Module):
    def __init__(self, feature_dim, num_experts, num_global_experts=4):
        super().__init__()
        self.feature_dim = feature_dim
        self.num_experts = num_experts
        self.num_global_experts = num_global_experts
        
        # Global router for high-level decisions
        self.global_router = nn.Linear(feature_dim, num_global_experts)
        self.global_steering = nn.Parameter(torch.randn(num_global_experts, feature_dim))
        
        # Local routers for fine-grained decisions
        self.local_routers = nn.ModuleList([
            nn.Linear(feature_dim, num_experts) for _ in range(num_global_experts)
        ])
        self.local_steering = nn.Parameter(torch.randn(num_global_experts, num_experts, feature_dim))
        
    def forward(self, h_audio):
        # Global routing
        global_logits = self.global_router(h_audio)  # (batch, seq_len, num_global_experts)
        global_scores = F.softmax(global_logits, dim=-1)
        
        # Local routing per global expert
        local_outputs = []
        for i in range(self.num_global_experts):
            local_logits = self.local_routers[i](h_audio)  # (batch, seq_len, num_experts)
            local_scores = F.softmax(local_logits, dim=-1)
            local_steering = torch.einsum('bte,ef->btf', local_scores, self.local_steering[i])
            local_outputs.append(local_steering)
        
        # Combine global and local steering
        global_steering = torch.einsum('bte,ef->btf', global_scores, self.global_steering)
        local_steering = torch.stack(local_outputs, dim=-1)  # (batch, seq_len, feature_dim, num_global_experts)
        local_steering = torch.einsum('btfe,bte->btf', local_steering, global_scores)
        
        return h_audio + global_steering + local_steering
```

**Pros:**
- Hierarchical decision making (global â†’ local)
- Better modeling of complex alignment patterns
- Can capture both coarse and fine-grained adjustments
- More interpretable routing decisions

**Cons:**
- Higher computational cost
- More complex training dynamics
- Potential overfitting with more parameters
- Harder to debug and analyze

### Progressive Router Design
```python
class ProgressiveRouter(nn.Module):
    def __init__(self, feature_dim, num_experts, num_stages=3):
        super().__init__()
        self.feature_dim = feature_dim
        self.num_experts = num_experts
        self.num_stages = num_stages
        
        # Progressive routers that build on previous decisions
        self.stage_routers = nn.ModuleList([
            nn.Linear(feature_dim + i * num_experts, num_experts) 
            for i in range(num_stages)
        ])
        self.stage_steering = nn.Parameter(torch.randn(num_stages, num_experts, feature_dim))
        
    def forward(self, h_audio):
        current_input = h_audio
        accumulated_steering = 0
        
        for stage in range(self.num_stages):
            # Route based on current input (includes previous steering info)
            stage_logits = self.stage_routers[stage](current_input)
            stage_scores = F.softmax(stage_logits, dim=-1)
            
            # Apply stage-specific steering
            stage_steering = torch.einsum('bte,ef->btf', stage_scores, self.stage_steering[stage])
            accumulated_steering += stage_steering
            
            # Update input for next stage
            current_input = torch.cat([h_audio, accumulated_steering], dim=-1)
        
        return h_audio + accumulated_steering
```

**Pros:**
- Progressive refinement of steering decisions
- Can correct previous routing mistakes
- Better handling of complex alignment patterns
- Temporal awareness in decision making

**Cons:**
- Sequential computation (not parallelizable)
- Higher memory usage
- More complex gradient flow
- Potential instability in early stages

## 2. Cross-Modal Attention Steering Analysis

### Your Concern: Feature Space Gap

You're absolutely correct! The frozen LLM decoder cannot directly understand the attended audio features. Here's the analysis:

**The Problem:**
```python
# This approach has a fundamental issue:
attended_audio, _ = self.cross_attention(audio_features, text_features, text_features)
# The LLM decoder expects features in its own learned space, not cross-attended audio
```

**Why `text_features` is used twice:**
The cross-attention mechanism follows the query-key-value pattern:
- `query`: `audio_features` (what we're looking for)
- `key`: `text_features` (what we're matching against)
- `value`: `text_features` (what we're retrieving)

This creates a "text-conditioned audio representation" but doesn't bridge the feature space gap.

### Better Cross-Modal Steering Approaches

#### 1. Feature Space Alignment
```python
class CrossModalFeatureAligner(nn.Module):
    def __init__(self, audio_dim, text_dim, hidden_dim):
        super().__init__()
        self.audio_projection = nn.Linear(audio_dim, hidden_dim)
        self.text_projection = nn.Linear(text_dim, hidden_dim)
        self.cross_attention = nn.MultiheadAttention(hidden_dim, num_heads=8)
        self.alignment_layer = nn.Linear(hidden_dim, text_dim)  # Project to LLM space
        
    def forward(self, audio_features, text_features):
        # Project both modalities to shared space
        audio_proj = self.audio_projection(audio_features)
        text_proj = self.text_projection(text_features)
        
        # Cross-attention in shared space
        attended_features, _ = self.cross_attention(
            audio_proj, text_proj, text_proj
        )
        
        # Project back to LLM-compatible space
        aligned_audio = self.alignment_layer(attended_features)
        
        return aligned_audio
```

#### 2. Modality-Specific Steering with Fusion
```python
class ModalitySpecificSteering(nn.Module):
    def __init__(self, audio_dim, text_dim, num_experts):
        super().__init__()
        self.audio_router = nn.Linear(audio_dim, num_experts)
        self.text_router = nn.Linear(text_dim, num_experts)
        self.audio_steering = nn.Parameter(torch.randn(num_experts, audio_dim))
        self.text_steering = nn.Parameter(torch.randn(num_experts, text_dim))
        self.fusion_layer = nn.Linear(audio_dim + text_dim, text_dim)
        
    def forward(self, audio_features, text_features):
        # Audio-specific steering
        audio_gating = F.softmax(self.audio_router(audio_features), dim=-1)
        audio_adjustment = torch.einsum('bte,ef->btf', audio_gating, self.audio_steering)
        steered_audio = audio_features + audio_adjustment
        
        # Text-specific steering
        text_gating = F.softmax(self.text_router(text_features), dim=-1)
        text_adjustment = torch.einsum('bte,ef->btf', text_gating, self.text_steering)
        steered_text = text_features + text_adjustment
        
        # Fusion to LLM-compatible space
        fused_features = torch.cat([steered_audio, steered_text], dim=-1)
        llm_compatible = self.fusion_layer(fused_features)
        
        return llm_compatible
```

#### 3. Adapter-Based Cross-Modal Alignment
```python
class CrossModalAdapter(nn.Module):
    def __init__(self, audio_dim, text_dim, adapter_dim):
        super().__init__()
        self.audio_adapter = nn.Sequential(
            nn.Linear(audio_dim, adapter_dim),
            nn.ReLU(),
            nn.Linear(adapter_dim, text_dim)
        )
        self.cross_attention = nn.MultiheadAttention(text_dim, num_heads=8)
        
    def forward(self, audio_features, text_features):
        # Adapt audio to text space
        adapted_audio = self.audio_adapter(audio_features)
        
        # Cross-attention between adapted audio and text
        attended_audio, _ = self.cross_attention(
            adapted_audio, text_features, text_features
        )
        
        return attended_audio
```

## 3. Recommendations

### For Layer-Wise Steering:
1. **Start with Progressive Router** - Better for complex alignment patterns
2. **Use Hierarchical Router** if you need interpretable routing decisions
3. **Keep Current Simple Router** for efficiency and stability

### For Cross-Modal Alignment:
1. **Use Feature Space Alignment** - Most principled approach
2. **Consider Modality-Specific Steering** - Better for heterogeneous modalities
3. **Avoid direct cross-attention** without proper feature space bridging

### Key Insights:
- The frozen LLM decoder constraint is real and important
- Cross-modal attention without proper alignment is problematic
- Progressive routers offer better expressiveness for complex patterns
- Feature space alignment is crucial for multimodal integration 