# LoRA Model Configuration for Evaluation
# Ablation study baseline for comparison with SteerMoE

# Whisper Encoder Configuration
whisper_encoder:
  model_path: "/home/fengruitao/models/openai/whisper-large-v3/"
  feature_dim: 1280
  num_layers: 32

# LLM Decoder Configuration
llm_decoder:
  model_name: "/home/fengruitao/models/Qwen2.5-7B-Instruct/"
  max_length: 512
  use_cache: false  # Disable cache during evaluation

# LoRA Configuration (must match training config)
lora:
  rank: 8  # LoRA rank (low-rank dimension)
  alpha: 1.0  # LoRA scaling factor (alpha/rank)
  dropout: 0.0  # LoRA dropout rate
  target_modules:  # Modules to apply LoRA to (must match training)
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "out_proj"
    - "fc1"
    - "fc2"

# Training Configuration (for evaluation settings)
training:
  output_dir: "/home/fengruitao/SteerMoE/results/lora_qwen7b_whisper_libri"
  logging_dir: "/home/fengruitao/SteerMoE/logs/lora_qwen7b_whisper_libri"
  batch_size: 4
  epochs: 320  # Not used in evaluation, but kept for compatibility
  learning_rate: 1e-4
  lora_learning_rate: 1e-3
  weight_decay: 0.01
  warmup_steps: 1000
  gradient_clipping: 1.0
  fp16: true
  use_deepspeed: true
  per_device_eval_batch_size: 1  # Evaluation batch size

# Dataset Configuration
parquet_dirs:
  - "/home/fengruitao/rt_nas/data/librispeech_asr/all/test.clean/"

audio_column: "audio"
text_column: "text"
sample_rate: 16000
max_audio_length: 30.0  # seconds
max_text_length: 448
filter_dataset: true

# Textual Prompt Configuration
textual_prompt: "please transcribe the audio content into text: "

# Model Configuration
max_prompt_tokens: 2048
use_adapter: true
save_total_limit: 2
logging_steps: 100
eval_steps: 1000
save_steps: 1000

# Callbacks Configuration (not used in evaluation)
log_lora_analysis: false
lora_log_interval: 100
clip_lora_gradients: false
lora_gradient_clip: 1.0

# DeepSpeed Configuration (if using)
deepspeed_config: "configs/stage2.json"

# Logging Configuration
report_to: ["none"]
logging_strategy: "steps"
save_strategy: "steps"

# Pooling Configuration (must match training config)
pooling_kernel_size: 4
pooling_position: 32
pooling_type: "avg"
